{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3t1TFdQk4yL",
        "outputId": "8d8717ce-3196-4bc7-e4c5-28e9051842a6",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Step 1: Import Libraries and Enable Mixed Precision\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, applications\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# --- KEY FOR SPEED: Enable Mixed Precision Training ---\n",
        "try:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print(\"‚úÖ Mixed precision enabled.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not enable mixed precision: {e}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Step 2: Load CIFAR-10 Dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Step 3: Create an Enhanced tf.data Pipeline\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 64\n",
        "IMG_SIZE = 32\n",
        "\n",
        "# --- NEW: More Powerful Data Augmentation ---\n",
        "def random_cutout(image, mask_size_factor=0.5):\n",
        "    \"\"\"Applies Random Cutout augmentation.\"\"\"\n",
        "    input_shape = tf.shape(image)\n",
        "    h, w = input_shape[0], input_shape[1]\n",
        "\n",
        "    mask_h = tf.cast(tf.cast(h, tf.float32) * mask_size_factor, tf.int32)\n",
        "    mask_w = tf.cast(tf.cast(w, tf.float32) * mask_size_factor, tf.int32)\n",
        "\n",
        "    # Randomly select top-left corner\n",
        "    offset_h = tf.random.uniform([], maxval=h - mask_h, dtype=tf.int32)\n",
        "    offset_w = tf.random.uniform([], maxval=w - mask_w, dtype=tf.int32)\n",
        "\n",
        "    # Create the cutout patch (zeros)\n",
        "    cutout_patch = tf.zeros([mask_h, mask_w, 3], dtype=image.dtype)\n",
        "\n",
        "    # Pad the cutout patch to image size\n",
        "    padded_patch = tf.image.pad_to_bounding_box(cutout_patch, offset_h, offset_w, h, w)\n",
        "\n",
        "    # Create a mask to apply cutout\n",
        "    mask = tf.equal(padded_patch, 0)\n",
        "\n",
        "    return tf.where(mask, image, 0)\n",
        "\n",
        "def augment_data(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "\n",
        "    # Apply cutout with a 50% probability\n",
        "    if tf.random.uniform([]) > 0.5:\n",
        "        image = random_cutout(image)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_images))\n",
        "train_dataset = train_dataset.map(augment_data, num_parallel_calls=AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Step 4: Build the Improved Model\n",
        "def build_model():\n",
        "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # --- IMPROVED: Upsample to 64x64 for more detail ---\n",
        "    # Ensure the size is an integer tuple\n",
        "    x = layers.UpSampling2D(size=(64 // IMG_SIZE, 64 // IMG_SIZE), interpolation='bilinear')(inputs)\n",
        "\n",
        "    base_model = applications.EfficientNetB0(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=x,\n",
        "        pooling='avg'\n",
        "    )\n",
        "\n",
        "    # --- IMPROVED: Progressive Fine-Tuning ---\n",
        "    # Freeze the entire base model initially\n",
        "    base_model.trainable = False\n",
        "    # Unfreeze the top 20 layers for fine-tuning\n",
        "    for layer in base_model.layers[-20:]:\n",
        "        # The BatchNormalization layers should be kept frozen\n",
        "        if not isinstance(layer, layers.BatchNormalization):\n",
        "            layer.trainable = True\n",
        "\n",
        "    # Build the classification head\n",
        "    x = layers.BatchNormalization()(base_model.output)\n",
        "    # --- IMPROVED: Increased dropout for better regularization ---\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(10, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()\n",
        "\n",
        "# Step 5: Compile and Train with an Advanced Learning Rate Schedule\n",
        "EPOCHS = 5 # Set a max number of epochs, EarlyStopping will handle the rest\n",
        "\n",
        "# --- IMPROVED: Learning Rate Schedule for combined training ---\n",
        "# Phase 1: Higher LR for 5 epochs to train the head\n",
        "# Phase 2: Lower LR for the remaining epochs to fine-tune\n",
        "boundaries = [5 * len(train_dataset)]\n",
        "values = [1e-3, 1e-4]\n",
        "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
        "\n",
        "# Compile the model ONCE\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# --- IMPROVED: Single training run with EarlyStopping ---\n",
        "# This stops training when validation loss stops improving, ensuring efficiency\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=test_dataset,\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Step 6: Evaluate the Final Model\n",
        "print(\"\\n--- Evaluating the final model on the test set ---\")\n",
        "test_loss, test_acc = model.evaluate(test_dataset, verbose=2)\n",
        "print(f\"‚úÖ Final Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_duration = end_time - start_time\n",
        "print(f\"‚è±Ô∏è Total script duration: {training_duration / 60:.2f} minutes\")\n",
        "\n",
        "# Step 7: Plot Training History\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 8: Save Model Performance Report\n",
        "try:\n",
        "    best_epoch_idx = np.argmin(history.history['val_loss'])\n",
        "    best_val_loss = history.history['val_loss'][best_epoch_idx]\n",
        "    best_val_acc = history.history['val_accuracy'][best_epoch_idx]\n",
        "\n",
        "    performance_text = f\"\"\"Model Performance Summary (Optimized EfficientNetB0):\n",
        "Total Script Duration: {training_duration / 60:.2f} minutes\n",
        "\n",
        "--- Final Evaluation on Test Set (Restored Best Weights) ---\n",
        "Test Accuracy: {test_acc:.4f}\n",
        "Test Loss: {test_loss:.4f}\n",
        "\n",
        "--- Metrics at Best Epoch ({best_epoch_idx + 1}) ---\n",
        "Best Validation Loss: {best_val_loss:.4f}\n",
        "Best Validation Accuracy: {best_val_acc:.4f}\n",
        "\n",
        "--- Training Details ---\n",
        "Total Epochs Run: {len(history.history['loss'])}\n",
        "Model: EfficientNetB0\n",
        "Techniques: Transfer Learning, Progressive Fine-Tuning, Mixed Precision, Cutout Augmentation\n",
        "Model Parameters: {model.count_params()}\"\"\"\n",
        "\n",
        "    with open('model_accuracy_optimized.txt', 'w') as f:\n",
        "        f.write(performance_text)\n",
        "\n",
        "    print(\"\\nüìÑ Model performance saved to model_accuracy_optimized.txt\")\n",
        "    print(performance_text)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error saving model performance: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
